{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 1,
>>>>>>> 3e002b6 (Solo se dio una corrida al notebook)
   "id": "e3faf33c-3347-4d14-98b2-df853340a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:55:16.344266: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-05 17:55:16.523528: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-05 17:55:17.262661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 17:55:18.622763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>production_date</th>\n",
       "      <th>genres</th>\n",
       "      <th>runtime_minutes</th>\n",
       "      <th>director_name</th>\n",
       "      <th>director_professions</th>\n",
       "      <th>director_birthYear</th>\n",
       "      <th>director_deathYear</th>\n",
       "      <th>movie_averageRating</th>\n",
       "      <th>movie_numerOfVotes</th>\n",
       "      <th>approval_Index</th>\n",
       "      <th>Production budget $</th>\n",
       "      <th>Domestic gross $</th>\n",
       "      <th>Worldwide gross $</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar: The Way of Water</td>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>Action,Adventure,Fantasy</td>\n",
       "      <td>192.0</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>writer,producer,director</td>\n",
       "      <td>1954</td>\n",
       "      <td>alive</td>\n",
       "      <td>7.8</td>\n",
       "      <td>277543.0</td>\n",
       "      <td>7.061101</td>\n",
       "      <td>460000000</td>\n",
       "      <td>667830256</td>\n",
       "      <td>2265935552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>Action,Adventure,Drama</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1143642.0</td>\n",
       "      <td>8.489533</td>\n",
       "      <td>400000000</td>\n",
       "      <td>858373000</td>\n",
       "      <td>2794731755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pirates of the Caribbean: On Stranger Tides</td>\n",
       "      <td>2011-05-20</td>\n",
       "      <td>Action,Adventure,Fantasy</td>\n",
       "      <td>137.0</td>\n",
       "      <td>Rob Marshall</td>\n",
       "      <td>director,miscellaneous,producer</td>\n",
       "      <td>1960</td>\n",
       "      <td>alive</td>\n",
       "      <td>6.6</td>\n",
       "      <td>533763.0</td>\n",
       "      <td>6.272064</td>\n",
       "      <td>379000000</td>\n",
       "      <td>241071802</td>\n",
       "      <td>1045713802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avengers: Age of Ultron</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>141.0</td>\n",
       "      <td>Joss Whedon</td>\n",
       "      <td>writer,producer,director</td>\n",
       "      <td>1964</td>\n",
       "      <td>alive</td>\n",
       "      <td>7.3</td>\n",
       "      <td>870573.0</td>\n",
       "      <td>7.214013</td>\n",
       "      <td>365000000</td>\n",
       "      <td>459005868</td>\n",
       "      <td>1395316979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>149.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1091968.0</td>\n",
       "      <td>8.460958</td>\n",
       "      <td>300000000</td>\n",
       "      <td>678815482</td>\n",
       "      <td>2048359754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   movie_title production_date  \\\n",
       "0                     Avatar: The Way of Water      2022-12-09   \n",
       "1                            Avengers: Endgame      2019-04-23   \n",
       "2  Pirates of the Caribbean: On Stranger Tides      2011-05-20   \n",
       "3                      Avengers: Age of Ultron      2015-04-22   \n",
       "4                       Avengers: Infinity War      2018-04-25   \n",
       "\n",
       "                     genres  runtime_minutes  director_name  \\\n",
       "0  Action,Adventure,Fantasy            192.0  James Cameron   \n",
       "1    Action,Adventure,Drama            181.0              -   \n",
       "2  Action,Adventure,Fantasy            137.0   Rob Marshall   \n",
       "3   Action,Adventure,Sci-Fi            141.0    Joss Whedon   \n",
       "4   Action,Adventure,Sci-Fi            149.0              -   \n",
       "\n",
       "              director_professions director_birthYear director_deathYear  \\\n",
       "0         writer,producer,director               1954              alive   \n",
       "1                                -                  -                  -   \n",
       "2  director,miscellaneous,producer               1960              alive   \n",
       "3         writer,producer,director               1964              alive   \n",
       "4                                -                  -                  -   \n",
       "\n",
       "   movie_averageRating  movie_numerOfVotes  approval_Index  \\\n",
       "0                  7.8            277543.0        7.061101   \n",
       "1                  8.4           1143642.0        8.489533   \n",
       "2                  6.6            533763.0        6.272064   \n",
       "3                  7.3            870573.0        7.214013   \n",
       "4                  8.4           1091968.0        8.460958   \n",
       "\n",
       "   Production budget $  Domestic gross $  Worldwide gross $  \n",
       "0            460000000         667830256         2265935552  \n",
       "1            400000000         858373000         2794731755  \n",
       "2            379000000         241071802         1045713802  \n",
       "3            365000000         459005868         1395316979  \n",
       "4            300000000         678815482         2048359754  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 21,
=======
     "execution_count": 1,
>>>>>>> 3e002b6 (Solo se dio una corrida al notebook)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "for package in [\"keras\", \"numpy\", \"pandas\", \"tensorflow\", \"scikit-learn\"]:\n",
    "\tsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Leer el dataset\n",
    "df = pd.read_csv('movie_statistic_dataset.csv')\n",
    "\n",
    "# Analizar los datos\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 2,
>>>>>>> 3e002b6 (Solo se dio una corrida al notebook)
   "id": "3e1678ef-9826-4655-954b-a33af5a6a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4380 entries, 0 to 4379\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   movie_title           4380 non-null   object \n",
      " 1   production_date       4380 non-null   object \n",
      " 2   genres                4380 non-null   object \n",
      " 3   runtime_minutes       4380 non-null   float64\n",
      " 4   director_name         4380 non-null   object \n",
      " 5   director_professions  4380 non-null   object \n",
      " 6   director_birthYear    4380 non-null   object \n",
      " 7   director_deathYear    4380 non-null   object \n",
      " 8   movie_averageRating   4380 non-null   float64\n",
      " 9   movie_numerOfVotes    4380 non-null   float64\n",
      " 10  approval_Index        4380 non-null   float64\n",
      " 11  Production budget $   4380 non-null   int64  \n",
      " 12  Domestic gross $      4380 non-null   int64  \n",
      " 13  Worldwide gross $     4380 non-null   int64  \n",
      "dtypes: float64(4), int64(3), object(7)\n",
      "memory usage: 479.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 3,
>>>>>>> 3e002b6 (Solo se dio una corrida al notebook)
   "id": "6571bd4a-69f9-4d49-82f4-d7b08cd2f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3504, 318)\n",
      "y_train shape: (3504,)\n",
      "X_test shape: (876, 318)\n",
      "y_test shape: (876,)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "# Seleccionar características y objetivo\n",
    "features = ['runtime_minutes', 'movie_averageRating', 'movie_numerOfVotes', 'approval_Index', 'Production budget $', 'Domestic gross $', 'genres']\n",
    "target = 'Worldwide gross $'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "numeric_features = ['runtime_minutes', 'movie_averageRating', 'movie_numerOfVotes', 'approval_Index', 'Production budget $', 'Domestic gross $']\n",
    "categorical_features = ['genres']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('num', numeric_transformer, numeric_features),\n",
    "\t\t('cat', categorical_transformer, categorical_features)\n",
    "\t])\n",
    "\n",
    "# Crear y dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar la transformación\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 4,
>>>>>>> 3e002b6 (Solo se dio una corrida al notebook)
   "id": "8ddbbb92-3470-49ed-b5d1-b5078be2e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "c:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=  (0, 0)\t0.29970697015284004\n  (0, 1)\t-0.09678766554139802\n  (0, 2)\t-0.5928605320490684\n  (0, 3)\t-0.861358431775164\n  (0, 4)\t-0.42188832552082833\n  (0, 5)\t-0.676317798355686\n  (0, 213)\t1.0\n  (1, 0)\t0.20264844673180937\n  (1, 1)\t-0.19427084463224586\n  (1, 2)\t-0.5142687038047253\n  (1, 3)\t-0.4196995368610382\n  (1, 4)\t-0.18046476470984202\n  (1, 5)\t-0.01733463218874696\n  (1, 274)\t1.0\n  (2, 0)\t0.6879410638369625\n  (2, 1)\t0.6830777671853873\n  (2, 2)\t0.04985461290148334\n  (2, 3)\t0.7676401925017781\n  (2, 4)\t0.2584871640374058\n  (2, 5)\t0.32809423234075225\n  (2, 161)\t1.0\n  (3, 0)\t0.7364703255474779\n  (3, 1)\t0.5855945880945395\n  (3, 2)\t-0.10955623127152007\n  (3, 3)\t0.6026844472007026\n  :\t:\n  (3500, 3)\t-0.3802021022770585\n  (3500, 4)\t1.1363910215319015\n  (3500, 5)\t0.12862112949279475\n  (3500, 121)\t1.0\n  (3501, 0)\t0.5908825404159319\n  (3501, 1)\t1.0730104835487804\n  (3501, 2)\t-0.1731888584575421\n  (3501, 3)\t0.8577895904471136\n  (3501, 4)\t-0.5974690970197275\n  (3501, 5)\t-0.1619662247220614\n  (3501, 208)\t1.0\n  (3502, 0)\t-0.37970269379437444\n  (3502, 1)\t0.5855945880945395\n  (3502, 2)\t-0.03398276826670379\n  (3502, 3)\t0.6544136764709031\n  (3502, 4)\t-0.740128473862583\n  (3502, 5)\t-0.1422803741137938\n  (3502, 306)\t1.0\n  (3503, 0)\t0.2511777084423247\n  (3503, 1)\t0.19566187173114635\n  (3503, 2)\t-0.4795300638661726\n  (3503, 3)\t-0.10395524041634999\n  (3503, 4)\t0.47796312841102967\n  (3503, 5)\t0.07230003613163077\n  (3503, 238)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model1\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m history1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluar\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss1, mae1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32mc:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:113\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=  (0, 0)\t0.29970697015284004\n  (0, 1)\t-0.09678766554139802\n  (0, 2)\t-0.5928605320490684\n  (0, 3)\t-0.861358431775164\n  (0, 4)\t-0.42188832552082833\n  (0, 5)\t-0.676317798355686\n  (0, 213)\t1.0\n  (1, 0)\t0.20264844673180937\n  (1, 1)\t-0.19427084463224586\n  (1, 2)\t-0.5142687038047253\n  (1, 3)\t-0.4196995368610382\n  (1, 4)\t-0.18046476470984202\n  (1, 5)\t-0.01733463218874696\n  (1, 274)\t1.0\n  (2, 0)\t0.6879410638369625\n  (2, 1)\t0.6830777671853873\n  (2, 2)\t0.04985461290148334\n  (2, 3)\t0.7676401925017781\n  (2, 4)\t0.2584871640374058\n  (2, 5)\t0.32809423234075225\n  (2, 161)\t1.0\n  (3, 0)\t0.7364703255474779\n  (3, 1)\t0.5855945880945395\n  (3, 2)\t-0.10955623127152007\n  (3, 3)\t0.6026844472007026\n  :\t:\n  (3500, 3)\t-0.3802021022770585\n  (3500, 4)\t1.1363910215319015\n  (3500, 5)\t0.12862112949279475\n  (3500, 121)\t1.0\n  (3501, 0)\t0.5908825404159319\n  (3501, 1)\t1.0730104835487804\n  (3501, 2)\t-0.1731888584575421\n  (3501, 3)\t0.8577895904471136\n  (3501, 4)\t-0.5974690970197275\n  (3501, 5)\t-0.1619662247220614\n  (3501, 208)\t1.0\n  (3502, 0)\t-0.37970269379437444\n  (3502, 1)\t0.5855945880945395\n  (3502, 2)\t-0.03398276826670379\n  (3502, 3)\t0.6544136764709031\n  (3502, 4)\t-0.740128473862583\n  (3502, 5)\t-0.1422803741137938\n  (3502, 306)\t1.0\n  (3503, 0)\t0.2511777084423247\n  (3503, 1)\t0.19566187173114635\n  (3503, 2)\t-0.4795300638661726\n  (3503, 3)\t-0.10395524041634999\n  (3503, 4)\t0.47796312841102967\n  (3503, 5)\t0.07230003613163077\n  (3503, 238)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)"
=======
      "/home/arg/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-05 17:55:19.856075: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-05 17:55:19.856889: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 56821940584710144.0000 - mae: 117018088.0000 - val_loss: 61200127167037440.0000 - val_mae: 127529584.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49769376651411456.0000 - mae: 109052688.0000 - val_loss: 61200088512331776.0000 - val_mae: 127529520.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 59276810747117568.0000 - mae: 119484600.0000 - val_loss: 61199951073378304.0000 - val_mae: 127529312.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51834525776347136.0000 - mae: 107485400.0000 - val_loss: 61199650425667584.0000 - val_mae: 127528896.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 45787229003448320.0000 - mae: 111261680.0000 - val_loss: 61199126439657472.0000 - val_mae: 127528136.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46817621722529792.0000 - mae: 109830880.0000 - val_loss: 61198383410315264.0000 - val_mae: 127527096.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 42603026149539840.0000 - mae: 106308768.0000 - val_loss: 61197417042673664.0000 - val_mae: 127525696.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 45079839299862528.0000 - mae: 107940968.0000 - val_loss: 61196111372615680.0000 - val_mae: 127523872.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49580333665878016.0000 - mae: 110521400.0000 - val_loss: 61194560889421824.0000 - val_mae: 127521656.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 43865802369138688.0000 - mae: 109305512.0000 - val_loss: 61192701168582656.0000 - val_mae: 127519000.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 54497547233787904.0000 - mae: 111004704.0000 - val_loss: 61190381886242816.0000 - val_mae: 127515680.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 46353297103126528.0000 - mae: 109685584.0000 - val_loss: 61187710416584704.0000 - val_mae: 127511864.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 57136207636725760.0000 - mae: 116168632.0000 - val_loss: 61184811313659904.0000 - val_mae: 127507720.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51117446626541568.0000 - mae: 111299552.0000 - val_loss: 61181422584463360.0000 - val_mae: 127502896.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49268815392931840.0000 - mae: 108939016.0000 - val_loss: 61177574293766144.0000 - val_mae: 127497416.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48245801427664896.0000 - mae: 110210168.0000 - val_loss: 61173257851633664.0000 - val_mae: 127491256.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51428440913477632.0000 - mae: 108433304.0000 - val_loss: 61168662236626944.0000 - val_mae: 127484664.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49177362654298112.0000 - mae: 111001336.0000 - val_loss: 61163456736264192.0000 - val_mae: 127477256.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 56313102924251136.0000 - mae: 114858048.0000 - val_loss: 61157899048583168.0000 - val_mae: 127469328.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 52231844610965504.0000 - mae: 113566696.0000 - val_loss: 61152045008158720.0000 - val_mae: 127460984.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 50913233816518656.0000 - mae: 113442240.0000 - val_loss: 61145624032051200.0000 - val_mae: 127451816.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 55728922947485696.0000 - mae: 113424456.0000 - val_loss: 61138992602546176.0000 - val_mae: 127442280.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51882960122544128.0000 - mae: 112535384.0000 - val_loss: 61131695453110272.0000 - val_mae: 127431840.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 64961754014154752.0000 - mae: 117891416.0000 - val_loss: 61123990281781248.0000 - val_mae: 127420792.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51987018590191616.0000 - mae: 111557984.0000 - val_loss: 61115761124442112.0000 - val_mae: 127409000.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53822309655379968.0000 - mae: 113733072.0000 - val_loss: 61107171189850112.0000 - val_mae: 127396688.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56459496884535296.0000 - mae: 114459424.0000 - val_loss: 61097700786962432.0000 - val_mae: 127383128.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53365149041426432.0000 - mae: 115505016.0000 - val_loss: 61088015635709952.0000 - val_mae: 127369264.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54201357699121152.0000 - mae: 113755536.0000 - val_loss: 61077651879624704.0000 - val_mae: 127354440.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51973184500531200.0000 - mae: 112222528.0000 - val_loss: 61067262353735680.0000 - val_mae: 127339536.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52140477771677696.0000 - mae: 109836104.0000 - val_loss: 61055940819943424.0000 - val_mae: 127323280.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48594986563796992.0000 - mae: 110456392.0000 - val_loss: 61044529091837952.0000 - val_mae: 127306896.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55039756790136832.0000 - mae: 118395984.0000 - val_loss: 61032219715567616.0000 - val_mae: 127289272.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43166551628578816.0000 - mae: 106650088.0000 - val_loss: 61019536677142528.0000 - val_mae: 127271128.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48287789027950592.0000 - mae: 109013312.0000 - val_loss: 61006029004996608.0000 - val_mae: 127251760.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49992586101784576.0000 - mae: 110775464.0000 - val_loss: 60991808368279552.0000 - val_mae: 127231440.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50657378319728640.0000 - mae: 111534232.0000 - val_loss: 60977639271170048.0000 - val_mae: 127211136.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 47006900931264512.0000 - mae: 108603424.0000 - val_loss: 60962473741647872.0000 - val_mae: 127189432.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 53780373594701824.0000 - mae: 114466040.0000 - val_loss: 60946414858928128.0000 - val_mae: 127166528.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49603230136532992.0000 - mae: 112384672.0000 - val_loss: 60930514889998336.0000 - val_mae: 127143704.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 56810528856604672.0000 - mae: 114343696.0000 - val_loss: 60913588423884800.0000 - val_mae: 127119512.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48715022309785600.0000 - mae: 112218008.0000 - val_loss: 60896966900449280.0000 - val_mae: 127095608.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47946373487656960.0000 - mae: 113322328.0000 - val_loss: 60878820663623680.0000 - val_mae: 127069632.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 46524966945947648.0000 - mae: 108501368.0000 - val_loss: 60859982937063424.0000 - val_mae: 127042640.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 60892246731390976.0000 - mae: 115162216.0000 - val_loss: 60840767253381120.0000 - val_mae: 127015128.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51267147711643648.0000 - mae: 113554152.0000 - val_loss: 60821242332053504.0000 - val_mae: 126987128.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50629413787664384.0000 - mae: 109717752.0000 - val_loss: 60800643668901888.0000 - val_mae: 126957608.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 55523065164988416.0000 - mae: 114350264.0000 - val_loss: 60779499544903680.0000 - val_mae: 126927296.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 55909698120974336.0000 - mae: 114089520.0000 - val_loss: 60758269521559552.0000 - val_mae: 126896760.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 45798868364820480.0000 - mae: 107392264.0000 - val_loss: 60736519807172608.0000 - val_mae: 126865536.0000\n",
      "Epoch 1/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 46700824381882368.0000 - mae: 109300760.0000 - val_loss: 61200127167037440.0000 - val_mae: 127529584.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49432573905993728.0000 - mae: 110040536.0000 - val_loss: 61200127167037440.0000 - val_mae: 127529576.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63648679432552448.0000 - mae: 117255400.0000 - val_loss: 61200118577102848.0000 - val_mae: 127529568.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55677735527251968.0000 - mae: 115425752.0000 - val_loss: 61200114282135552.0000 - val_mae: 127529552.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49874126608793600.0000 - mae: 112354280.0000 - val_loss: 61200114282135552.0000 - val_mae: 127529536.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50072627112312832.0000 - mae: 112433424.0000 - val_loss: 61200114282135552.0000 - val_mae: 127529536.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51711255919984640.0000 - mae: 111745344.0000 - val_loss: 61200114282135552.0000 - val_mae: 127529528.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52520079866200064.0000 - mae: 111781136.0000 - val_loss: 61200114282135552.0000 - val_mae: 127529520.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53726493229973504.0000 - mae: 112661456.0000 - val_loss: 61200114282135552.0000 - val_mae: 127529520.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53924151919902720.0000 - mae: 112437104.0000 - val_loss: 61200105692200960.0000 - val_mae: 127529504.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46755546560200704.0000 - mae: 106827304.0000 - val_loss: 61200105692200960.0000 - val_mae: 127529496.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53686395415298048.0000 - mae: 112527536.0000 - val_loss: 61200101397233664.0000 - val_mae: 127529496.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47607500568002560.0000 - mae: 110102400.0000 - val_loss: 61200101397233664.0000 - val_mae: 127529480.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51754059564056576.0000 - mae: 109178760.0000 - val_loss: 61200101397233664.0000 - val_mae: 127529472.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49746694929121280.0000 - mae: 113015456.0000 - val_loss: 61200101397233664.0000 - val_mae: 127529456.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51982848176947200.0000 - mae: 110392080.0000 - val_loss: 61200101397233664.0000 - val_mae: 127529456.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45348021352792064.0000 - mae: 106906584.0000 - val_loss: 61200097102266368.0000 - val_mae: 127529456.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48805865163063296.0000 - mae: 111190784.0000 - val_loss: 61200097102266368.0000 - val_mae: 127529456.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52258731106238464.0000 - mae: 110615520.0000 - val_loss: 61200097102266368.0000 - val_mae: 127529456.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52247933558456320.0000 - mae: 114496144.0000 - val_loss: 61200097102266368.0000 - val_mae: 127529456.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57219066145800192.0000 - mae: 117755840.0000 - val_loss: 61200097102266368.0000 - val_mae: 127529456.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49694669990264832.0000 - mae: 113869824.0000 - val_loss: 61200088512331776.0000 - val_mae: 127529440.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56340998736838656.0000 - mae: 114241456.0000 - val_loss: 61200088512331776.0000 - val_mae: 127529424.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53403327005720576.0000 - mae: 113467248.0000 - val_loss: 61200084217364480.0000 - val_mae: 127529424.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58704897196949504.0000 - mae: 117561720.0000 - val_loss: 61200084217364480.0000 - val_mae: 127529424.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58220205842628608.0000 - mae: 118080736.0000 - val_loss: 61200084217364480.0000 - val_mae: 127529408.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60765386282369024.0000 - mae: 115804960.0000 - val_loss: 61200075627429888.0000 - val_mae: 127529408.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53018832943448064.0000 - mae: 109526328.0000 - val_loss: 61200075627429888.0000 - val_mae: 127529400.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51468886620504064.0000 - mae: 112529144.0000 - val_loss: 61200075627429888.0000 - val_mae: 127529392.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48822138794147840.0000 - mae: 109494744.0000 - val_loss: 61200075627429888.0000 - val_mae: 127529392.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51695102547984384.0000 - mae: 112592280.0000 - val_loss: 61200075627429888.0000 - val_mae: 127529368.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61222091629789184.0000 - mae: 116820728.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529368.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48351564997328896.0000 - mae: 109682864.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529344.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44234679930322944.0000 - mae: 110167608.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529344.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46970062996766720.0000 - mae: 111785936.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529344.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55902057374154752.0000 - mae: 114866856.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529344.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62515838858559488.0000 - mae: 113776352.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529328.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49107264493060096.0000 - mae: 109119792.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529328.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46780745133326336.0000 - mae: 107764784.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529312.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50494100843003904.0000 - mae: 112013824.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529312.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54215492436492288.0000 - mae: 116044632.0000 - val_loss: 61200071332462592.0000 - val_mae: 127529296.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43019500538298368.0000 - mae: 104089176.0000 - val_loss: 61200067037495296.0000 - val_mae: 127529296.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56580937084829696.0000 - mae: 114350296.0000 - val_loss: 61200067037495296.0000 - val_mae: 127529288.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41054535820509184.0000 - mae: 104257616.0000 - val_loss: 61200067037495296.0000 - val_mae: 127529280.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52968070724976640.0000 - mae: 115619152.0000 - val_loss: 61200067037495296.0000 - val_mae: 127529272.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46434471985020928.0000 - mae: 108778184.0000 - val_loss: 61200067037495296.0000 - val_mae: 127529272.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44379381673492480.0000 - mae: 110045328.0000 - val_loss: 61200062742528000.0000 - val_mae: 127529248.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57998452386168832.0000 - mae: 116629472.0000 - val_loss: 61200062742528000.0000 - val_mae: 127529248.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47249721202311168.0000 - mae: 110117792.0000 - val_loss: 61200062742528000.0000 - val_mae: 127529248.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49610982552502272.0000 - mae: 113259360.0000 - val_loss: 61200058447560704.0000 - val_mae: 127529224.0000\n",
      "Epoch 1/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 53375474142806016.0000 - mae: 115621112.0000 - val_loss: 61199302533316608.0000 - val_mae: 127528256.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50002116634214400.0000 - mae: 109129024.0000 - val_loss: 61176354523054080.0000 - val_mae: 127494520.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59062066677284864.0000 - mae: 114498384.0000 - val_loss: 61039327886442496.0000 - val_mae: 127297632.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46057008784211968.0000 - mae: 105953152.0000 - val_loss: 60626783392759808.0000 - val_mae: 126707320.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49377263317155840.0000 - mae: 106015864.0000 - val_loss: 59792516060282880.0000 - val_mae: 125500552.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59160021996404736.0000 - mae: 115273328.0000 - val_loss: 58292915343982592.0000 - val_mae: 123335448.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43164434209701888.0000 - mae: 104361904.0000 - val_loss: 55861658911768576.0000 - val_mae: 119832480.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41258207464652800.0000 - mae: 99719208.0000 - val_loss: 52425212628566016.0000 - val_mae: 114846688.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43128330714611712.0000 - mae: 99867760.0000 - val_loss: 48187931038318592.0000 - val_mae: 108768360.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43138604276383744.0000 - mae: 96979728.0000 - val_loss: 42961286551568384.0000 - val_mae: 101499360.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34260836008591360.0000 - mae: 85922056.0000 - val_loss: 37559802536132608.0000 - val_mae: 94218864.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29926941276504064.0000 - mae: 80802192.0000 - val_loss: 31966073637044224.0000 - val_mae: 86807344.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24418299974516736.0000 - mae: 72842672.0000 - val_loss: 27218438262882304.0000 - val_mae: 80811624.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27034697414475776.0000 - mae: 74333720.0000 - val_loss: 23155356251193344.0000 - val_mae: 76140656.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20922213775441920.0000 - mae: 69705696.0000 - val_loss: 20056870207094784.0000 - val_mae: 72734760.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17263411666616320.0000 - mae: 66414504.0000 - val_loss: 17798695588200448.0000 - val_mae: 70180608.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18374312473919488.0000 - mae: 64638000.0000 - val_loss: 16128827335901184.0000 - val_mae: 68100936.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12925845811232768.0000 - mae: 60781420.0000 - val_loss: 14843291010859008.0000 - val_mae: 66251748.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12093914940964864.0000 - mae: 60916744.0000 - val_loss: 13834335547293696.0000 - val_mae: 64597280.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13136082312888320.0000 - mae: 58297320.0000 - val_loss: 13013706883465216.0000 - val_mae: 63006792.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10838695716323328.0000 - mae: 56344860.0000 - val_loss: 12306418111610880.0000 - val_mae: 61413036.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11910373808537600.0000 - mae: 58365540.0000 - val_loss: 11696788306132992.0000 - val_mae: 59866688.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10129904779657216.0000 - mae: 54927684.0000 - val_loss: 11140334490746880.0000 - val_mae: 58393960.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10377943435968512.0000 - mae: 52960692.0000 - val_loss: 10631928474501120.0000 - val_mae: 56927864.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9378717147070464.0000 - mae: 51195376.0000 - val_loss: 10171309338132480.0000 - val_mae: 55518392.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8909481232564224.0000 - mae: 49896444.0000 - val_loss: 9777913956139008.0000 - val_mae: 54149044.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11431357179756544.0000 - mae: 51241544.0000 - val_loss: 9428325093081088.0000 - val_mae: 52878444.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8788683666751488.0000 - mae: 51933164.0000 - val_loss: 9006642519605248.0000 - val_mae: 51893220.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7779697064673280.0000 - mae: 48237576.0000 - val_loss: 8684558627110912.0000 - val_mae: 50791900.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6811594256285696.0000 - mae: 44978400.0000 - val_loss: 8447398049218560.0000 - val_mae: 49665544.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6642423010689024.0000 - mae: 42917040.0000 - val_loss: 8115428752621568.0000 - val_mae: 48840896.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6700749740310528.0000 - mae: 43876388.0000 - val_loss: 7893011723714560.0000 - val_mae: 47801016.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6625009602658304.0000 - mae: 43871508.0000 - val_loss: 7670426117341184.0000 - val_mae: 46951060.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6415206892699648.0000 - mae: 41919212.0000 - val_loss: 7446327608737792.0000 - val_mae: 46237208.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6510483762839552.0000 - mae: 42431296.0000 - val_loss: 7273266733383680.0000 - val_mae: 45469252.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6383413934161920.0000 - mae: 41628564.0000 - val_loss: 7080297610870784.0000 - val_mae: 44912872.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6496611018473472.0000 - mae: 40227796.0000 - val_loss: 6930984146567168.0000 - val_mae: 44330312.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5863314332581888.0000 - mae: 39965228.0000 - val_loss: 6798919807795200.0000 - val_mae: 43750092.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6436096539885568.0000 - mae: 40382204.0000 - val_loss: 6664517933072384.0000 - val_mae: 43298492.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5862577208819712.0000 - mae: 37817436.0000 - val_loss: 6545066839506944.0000 - val_mae: 42900376.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5283498579460096.0000 - mae: 38207992.0000 - val_loss: 6424861442310144.0000 - val_mae: 42520960.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5207525976702976.0000 - mae: 39037592.0000 - val_loss: 6333840347889664.0000 - val_mae: 42094648.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5683773962190848.0000 - mae: 38562672.0000 - val_loss: 6231177207742464.0000 - val_mae: 41776988.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5246371808411648.0000 - mae: 36939152.0000 - val_loss: 6145530728022016.0000 - val_mae: 41479876.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5534376746024960.0000 - mae: 37583132.0000 - val_loss: 6038691269050368.0000 - val_mae: 41269564.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6149540616863744.0000 - mae: 38163404.0000 - val_loss: 5957050383204352.0000 - val_mae: 40988372.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4990458866434048.0000 - mae: 37130024.0000 - val_loss: 5887191129522176.0000 - val_mae: 40674472.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4877798653034496.0000 - mae: 36368600.0000 - val_loss: 5812817865211904.0000 - val_mae: 40397540.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4950096911269888.0000 - mae: 35211052.0000 - val_loss: 5744539159494656.0000 - val_mae: 40149368.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3995148513968128.0000 - mae: 33949972.0000 - val_loss: 5683005163044864.0000 - val_mae: 39868536.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 65833314547662848.0000 - mae: 135032192.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 66331702552690688.0000 - mae: 135719232.0000  \n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5743165306830848.0000 - mae: 41087940.0000 \n",
      "Model 1 - Loss: 6.073651980717261e+16, MAE: 126865536.0\n",
      "Model 2 - Loss: 6.12000584475607e+16, MAE: 127529224.0\n",
      "Model 3 - Loss: 5683005163044864.0, MAE: 39868536.0\n"
>>>>>>> 3e002b6 (Solo se dio una corrida al notebook)
     ]
    }
   ],
   "source": [
    "# Modelo 1\n",
    "model1 = Sequential([\n",
    "\tDense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "\tDropout(0.2),\n",
    "\tDense(32, activation='relu'),\n",
    "\tDense(1, activation='linear')\n",
    "])\n",
    "model1.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "# Entrenar\n",
    "history1 = model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "# Evaluar\n",
    "loss1, mae1 = model1.evaluate(X_test, y_test)\n",
    "print(f'Model 1 - Loss: {loss1}, MAE: {mae1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2109a-3869-490f-b3f7-b0267180b00c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=  (0, 0)\t0.29970697015284004\n  (0, 1)\t-0.09678766554139802\n  (0, 2)\t-0.5928605320490684\n  (0, 3)\t-0.861358431775164\n  (0, 4)\t-0.42188832552082833\n  (0, 5)\t-0.676317798355686\n  (0, 213)\t1.0\n  (1, 0)\t0.20264844673180937\n  (1, 1)\t-0.19427084463224586\n  (1, 2)\t-0.5142687038047253\n  (1, 3)\t-0.4196995368610382\n  (1, 4)\t-0.18046476470984202\n  (1, 5)\t-0.01733463218874696\n  (1, 274)\t1.0\n  (2, 0)\t0.6879410638369625\n  (2, 1)\t0.6830777671853873\n  (2, 2)\t0.04985461290148334\n  (2, 3)\t0.7676401925017781\n  (2, 4)\t0.2584871640374058\n  (2, 5)\t0.32809423234075225\n  (2, 161)\t1.0\n  (3, 0)\t0.7364703255474779\n  (3, 1)\t0.5855945880945395\n  (3, 2)\t-0.10955623127152007\n  (3, 3)\t0.6026844472007026\n  :\t:\n  (3500, 3)\t-0.3802021022770585\n  (3500, 4)\t1.1363910215319015\n  (3500, 5)\t0.12862112949279475\n  (3500, 121)\t1.0\n  (3501, 0)\t0.5908825404159319\n  (3501, 1)\t1.0730104835487804\n  (3501, 2)\t-0.1731888584575421\n  (3501, 3)\t0.8577895904471136\n  (3501, 4)\t-0.5974690970197275\n  (3501, 5)\t-0.1619662247220614\n  (3501, 208)\t1.0\n  (3502, 0)\t-0.37970269379437444\n  (3502, 1)\t0.5855945880945395\n  (3502, 2)\t-0.03398276826670379\n  (3502, 3)\t0.6544136764709031\n  (3502, 4)\t-0.740128473862583\n  (3502, 5)\t-0.1422803741137938\n  (3502, 306)\t1.0\n  (3503, 0)\t0.2511777084423247\n  (3503, 1)\t0.19566187173114635\n  (3503, 2)\t-0.4795300638661726\n  (3503, 3)\t-0.10395524041634999\n  (3503, 4)\t0.47796312841102967\n  (3503, 5)\t0.07230003613163077\n  (3503, 238)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model2\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluar\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss2, mae2 \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32mc:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:113\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=  (0, 0)\t0.29970697015284004\n  (0, 1)\t-0.09678766554139802\n  (0, 2)\t-0.5928605320490684\n  (0, 3)\t-0.861358431775164\n  (0, 4)\t-0.42188832552082833\n  (0, 5)\t-0.676317798355686\n  (0, 213)\t1.0\n  (1, 0)\t0.20264844673180937\n  (1, 1)\t-0.19427084463224586\n  (1, 2)\t-0.5142687038047253\n  (1, 3)\t-0.4196995368610382\n  (1, 4)\t-0.18046476470984202\n  (1, 5)\t-0.01733463218874696\n  (1, 274)\t1.0\n  (2, 0)\t0.6879410638369625\n  (2, 1)\t0.6830777671853873\n  (2, 2)\t0.04985461290148334\n  (2, 3)\t0.7676401925017781\n  (2, 4)\t0.2584871640374058\n  (2, 5)\t0.32809423234075225\n  (2, 161)\t1.0\n  (3, 0)\t0.7364703255474779\n  (3, 1)\t0.5855945880945395\n  (3, 2)\t-0.10955623127152007\n  (3, 3)\t0.6026844472007026\n  :\t:\n  (3500, 3)\t-0.3802021022770585\n  (3500, 4)\t1.1363910215319015\n  (3500, 5)\t0.12862112949279475\n  (3500, 121)\t1.0\n  (3501, 0)\t0.5908825404159319\n  (3501, 1)\t1.0730104835487804\n  (3501, 2)\t-0.1731888584575421\n  (3501, 3)\t0.8577895904471136\n  (3501, 4)\t-0.5974690970197275\n  (3501, 5)\t-0.1619662247220614\n  (3501, 208)\t1.0\n  (3502, 0)\t-0.37970269379437444\n  (3502, 1)\t0.5855945880945395\n  (3502, 2)\t-0.03398276826670379\n  (3502, 3)\t0.6544136764709031\n  (3502, 4)\t-0.740128473862583\n  (3502, 5)\t-0.1422803741137938\n  (3502, 306)\t1.0\n  (3503, 0)\t0.2511777084423247\n  (3503, 1)\t0.19566187173114635\n  (3503, 2)\t-0.4795300638661726\n  (3503, 3)\t-0.10395524041634999\n  (3503, 4)\t0.47796312841102967\n  (3503, 5)\t0.07230003613163077\n  (3503, 238)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)"
     ]
    }
   ],
   "source": [
    "# Modelo 2\n",
    "model2 = Sequential([\n",
    "\tDense(128, input_dim=X_train.shape[1], activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "\tDense(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "\tDense(1, activation='linear')\n",
    "])\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "# Entrenar\n",
    "history2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "# Evaluar\n",
    "loss2, mae2 = model2.evaluate(X_test, y_test)\n",
    "print(f'Model 2 - Loss: {loss2}, MAE: {mae2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a019b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=  (0, 0)\t0.29970697015284004\n  (0, 1)\t-0.09678766554139802\n  (0, 2)\t-0.5928605320490684\n  (0, 3)\t-0.861358431775164\n  (0, 4)\t-0.42188832552082833\n  (0, 5)\t-0.676317798355686\n  (0, 213)\t1.0\n  (1, 0)\t0.20264844673180937\n  (1, 1)\t-0.19427084463224586\n  (1, 2)\t-0.5142687038047253\n  (1, 3)\t-0.4196995368610382\n  (1, 4)\t-0.18046476470984202\n  (1, 5)\t-0.01733463218874696\n  (1, 274)\t1.0\n  (2, 0)\t0.6879410638369625\n  (2, 1)\t0.6830777671853873\n  (2, 2)\t0.04985461290148334\n  (2, 3)\t0.7676401925017781\n  (2, 4)\t0.2584871640374058\n  (2, 5)\t0.32809423234075225\n  (2, 161)\t1.0\n  (3, 0)\t0.7364703255474779\n  (3, 1)\t0.5855945880945395\n  (3, 2)\t-0.10955623127152007\n  (3, 3)\t0.6026844472007026\n  :\t:\n  (3500, 3)\t-0.3802021022770585\n  (3500, 4)\t1.1363910215319015\n  (3500, 5)\t0.12862112949279475\n  (3500, 121)\t1.0\n  (3501, 0)\t0.5908825404159319\n  (3501, 1)\t1.0730104835487804\n  (3501, 2)\t-0.1731888584575421\n  (3501, 3)\t0.8577895904471136\n  (3501, 4)\t-0.5974690970197275\n  (3501, 5)\t-0.1619662247220614\n  (3501, 208)\t1.0\n  (3502, 0)\t-0.37970269379437444\n  (3502, 1)\t0.5855945880945395\n  (3502, 2)\t-0.03398276826670379\n  (3502, 3)\t0.6544136764709031\n  (3502, 4)\t-0.740128473862583\n  (3502, 5)\t-0.1422803741137938\n  (3502, 306)\t1.0\n  (3503, 0)\t0.2511777084423247\n  (3503, 1)\t0.19566187173114635\n  (3503, 2)\t-0.4795300638661726\n  (3503, 3)\t-0.10395524041634999\n  (3503, 4)\t0.47796312841102967\n  (3503, 5)\t0.07230003613163077\n  (3503, 238)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model3\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m history3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluar\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss3, mae3 \u001b[38;5;241m=\u001b[39m model3\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32mc:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Programs\\Coding\\Python\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:113\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=  (0, 0)\t0.29970697015284004\n  (0, 1)\t-0.09678766554139802\n  (0, 2)\t-0.5928605320490684\n  (0, 3)\t-0.861358431775164\n  (0, 4)\t-0.42188832552082833\n  (0, 5)\t-0.676317798355686\n  (0, 213)\t1.0\n  (1, 0)\t0.20264844673180937\n  (1, 1)\t-0.19427084463224586\n  (1, 2)\t-0.5142687038047253\n  (1, 3)\t-0.4196995368610382\n  (1, 4)\t-0.18046476470984202\n  (1, 5)\t-0.01733463218874696\n  (1, 274)\t1.0\n  (2, 0)\t0.6879410638369625\n  (2, 1)\t0.6830777671853873\n  (2, 2)\t0.04985461290148334\n  (2, 3)\t0.7676401925017781\n  (2, 4)\t0.2584871640374058\n  (2, 5)\t0.32809423234075225\n  (2, 161)\t1.0\n  (3, 0)\t0.7364703255474779\n  (3, 1)\t0.5855945880945395\n  (3, 2)\t-0.10955623127152007\n  (3, 3)\t0.6026844472007026\n  :\t:\n  (3500, 3)\t-0.3802021022770585\n  (3500, 4)\t1.1363910215319015\n  (3500, 5)\t0.12862112949279475\n  (3500, 121)\t1.0\n  (3501, 0)\t0.5908825404159319\n  (3501, 1)\t1.0730104835487804\n  (3501, 2)\t-0.1731888584575421\n  (3501, 3)\t0.8577895904471136\n  (3501, 4)\t-0.5974690970197275\n  (3501, 5)\t-0.1619662247220614\n  (3501, 208)\t1.0\n  (3502, 0)\t-0.37970269379437444\n  (3502, 1)\t0.5855945880945395\n  (3502, 2)\t-0.03398276826670379\n  (3502, 3)\t0.6544136764709031\n  (3502, 4)\t-0.740128473862583\n  (3502, 5)\t-0.1422803741137938\n  (3502, 306)\t1.0\n  (3503, 0)\t0.2511777084423247\n  (3503, 1)\t0.19566187173114635\n  (3503, 2)\t-0.4795300638661726\n  (3503, 3)\t-0.10395524041634999\n  (3503, 4)\t0.47796312841102967\n  (3503, 5)\t0.07230003613163077\n  (3503, 238)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)"
     ]
    }
   ],
   "source": [
    "# Modelo 3\n",
    "model3 = Sequential([\n",
    "\tDense(256, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "\tDropout(0.2),\n",
    "\tDense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "\tDropout(0.2),\n",
    "\tDense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "\tDense(1, activation='linear')\n",
    "])\n",
    "model3.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "# Entrenar\n",
    "history3 = model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "# Evaluar\n",
    "loss3, mae3 = model3.evaluate(X_test, y_test)\n",
    "print(f'Model 3 - Loss: {loss3}, MAE: {mae3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
